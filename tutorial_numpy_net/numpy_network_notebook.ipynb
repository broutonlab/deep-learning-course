{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXAEiniU5UIZ"
      },
      "outputs": [],
      "source": [
        "!pip install ipyplot\n",
        "!pip install \"git+https://github.com/broutonlab/deep-learning-course.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LZ8hkL-dV_x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "import ipyplot\n",
        "import torchvision.transforms as transforms\n",
        "from typing import Tuple\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import dl_course.numpy_net.utils as courseutils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqp7b5WC_sbU"
      },
      "source": [
        "# CIFAR10 dataset\n",
        "\n",
        "In this notebook you'll build a classifier from scratch using numpy!  \n",
        "Today's dataset is CIFAR10 â€” a collection of 60000 32x32 images split into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFFgK1qqg39a"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Grayscale(),\n",
        "     transforms.Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "dataset_params = {\n",
        "    'root': './data',\n",
        "    'transform': transform,\n",
        "    'download': True\n",
        "}\n",
        "\n",
        "dataloader_params = {\n",
        "    # decrease the batch size if you're getting \"Out of memory\" error\n",
        "    'batch_size': 64, \n",
        "    'num_workers': 2\n",
        "}\n",
        "\n",
        "# Download the dataset using pytorch:\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(train=True, **dataset_params)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, shuffle=True,\n",
        "                                          **dataloader_params)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(train=False, **dataset_params)\n",
        "testloader = torch.utils.data.DataLoader(testset, shuffle=False,\n",
        "                                         **dataloader_params)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwS-nQdS32YL"
      },
      "outputs": [],
      "source": [
        "courseutils.preview_CIFAR10(dataset_params['root'], classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UVFHHmECHhP"
      },
      "source": [
        "# Sigmoid\n",
        "\n",
        "Have a look at Sigmoid activation function.  \n",
        "(https://en.wikipedia.org/wiki/Sigmoid_function)\n",
        "\n",
        "Like any other activation function, it is used to add non-linearity to models.\n",
        "\n",
        "As you (hopefully) already know, neural networks train using an algorithm called **backpropagation**. So, if you want to use something inside an architecture, it must have a **backward pass** to pass gradient from the node that comes after it to the one before it.\n",
        "\n",
        "You will be implementing backward passes for different layers today.  \n",
        "Look at how Sigmoid forward and backward passes are implemented:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV2xPEuHdiYq"
      },
      "outputs": [],
      "source": [
        "def sigmoid(X: np.ndarray):\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "def sigmoid_backward(d: np.ndarray, prev: np.ndarray):\n",
        "    X = prev.copy()\n",
        "    sigma = sigmoid(X)\n",
        "    dx = sigma * (1 - sigma) * d\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ia3aU2XvTk"
      },
      "source": [
        "# ReLU\n",
        "\n",
        "ReLU is another activation function. It has become immensely popular.  \n",
        "(https://en.wikipedia.org/wiki/Rectifier_(neural_networks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdqv1l9BX2Cu"
      },
      "outputs": [],
      "source": [
        "def relu(X: np.ndarray):\n",
        "    # the second return value is 'cache' for future use,\n",
        "    # you can ignore it for now\n",
        "    return np.maximum(0, X), X\n",
        "\n",
        "def relu_backward(d: np.ndarray, prev: np.ndarray):\n",
        "    X = prev.copy()\n",
        "    dx = d.copy()\n",
        "    # TODO: implement ReLU backpropagation here\n",
        "    ...\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQzzWtdsdKlw"
      },
      "source": [
        "# Linear layer\n",
        "\n",
        "Linear layer is basically a matrix multiplication between input and its inner state (its ***weights***) summed with ***bias***.  \n",
        "(just look at the forward pass if you're confused)\n",
        "\n",
        "It's also called **affine layer** or **fully-connected** layer and it learns affine transformation of the input.\n",
        "\n",
        "Your task is to write a backward pass with respect to input **X**, **W**eight and **b**ias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cejjxi2AbxGh"
      },
      "outputs": [],
      "source": [
        "def linear_backward(d: np.ndarray, prev: tuple):\n",
        "    X, W, b = prev\n",
        "    \n",
        "    # TODO: implement backpropagation for linear layer using numpy\n",
        "    ...\n",
        "\n",
        "    return dX, dW, db\n",
        "\n",
        "def linear(X: np.ndarray, W: np.ndarray, b: np.ndarray):\n",
        "    # the second return value is 'cache' for future use,\n",
        "    # you can ignore it for now\n",
        "    return np.dot(X, W) + b, \\\n",
        "           (X, W, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwx0yLAid3RZ"
      },
      "source": [
        "# Loss functions\n",
        "Softmax is a very common choice for classification tasks like ours.  \n",
        "We completed this one for you. Feel free to explore how this works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBOt-NuIdyXn"
      },
      "outputs": [],
      "source": [
        "def softmax(X: np.ndarray):\n",
        "    e_x = np.exp(X - np.max(X))\n",
        "    if len(e_x.shape) > 1:\n",
        "        return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
        "    return e_x / np.sum(e_x)\n",
        "\n",
        "\n",
        "def softmax_loss(X: np.ndarray, y: np.ndarray):\n",
        "    logits = X - np.max(X, axis=1, keepdims=True)\n",
        "    Z = np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "    log_probs = logits - np.log(Z)\n",
        "    probs = np.exp(log_probs)\n",
        "    N = X.shape[0]\n",
        "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
        "    dX = probs.copy()\n",
        "    dX[np.arange(N), y] -= 1\n",
        "    dX /= N\n",
        "    return loss, dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VImPk9Pje8BO"
      },
      "source": [
        "# Let's see if it works!\n",
        "\n",
        "We've implemented a very basic network:  \n",
        "`Linear(1024x500) -> ReLU -> Linear(500, 10) -> Softmax`  \n",
        "Let's see if your layers work properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ3yI81Ue2XD"
      },
      "outputs": [],
      "source": [
        "class NNet(courseutils.BaseNet):\n",
        "    def __init__(self, num_cls: int = 10, input_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        ## Architecture\n",
        "        self.params = {}\n",
        "        std = 1e-4\n",
        "        # first linear layer\n",
        "        self.params['W1'] = std * np.random.randn(input_dim, 500)\n",
        "        self.params['b1'] = np.zeros(500)\n",
        "\n",
        "        # second linear layer\n",
        "        self.params['W2'] = std * np.random.randn(500, num_cls)\n",
        "        self.params['b2'] = np.zeros(num_cls)\n",
        "\n",
        "        self.num_layers = 2\n",
        "        ## params\n",
        "        self.output_activation = softmax\n",
        "        self.loss = softmax_loss\n",
        "        self.optimizer = courseutils.Optimizer(\"adam\")\n",
        "        self.loss_history = []\n",
        "\n",
        "        ## datasets\n",
        "        self.labels2names = classes\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader = testloader\n",
        "        self.valloader = testloader\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> Tuple[list, dict]:\n",
        "        caches = {}\n",
        "\n",
        "        input = X.copy()\n",
        "\n",
        "        h1, cache1 = linear(input, self.params[\"W1\"], self.params[\"b1\"])\n",
        "        caches['linear1'] = cache1\n",
        "        h_relu, cache_relu = relu(h1)\n",
        "        caches['relu'] = cache_relu\n",
        "\n",
        "        output, cache = linear(h_relu, self.params['W2'], self.params['b2'])\n",
        "\n",
        "        caches[\"linear2\"] = cache\n",
        "\n",
        "        return output, caches\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, caches: dict) -> Tuple[float, dict]:\n",
        "        grads = {}\n",
        "        loss, dOut = self.loss(X, y)\n",
        "\n",
        "        dOut, grads[\"W2\"], grads[\"b2\"] = linear_backward(dOut, caches['linear2'])\n",
        "\n",
        "        dOut = relu_backward(dOut, caches['relu'])\n",
        "        dOut, grads['W1'], grads['b1'] = linear_backward(dOut, caches['linear1'])\n",
        "\n",
        "        return loss, grads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz9zfsHle-Q2"
      },
      "source": [
        "# Train the model!\n",
        "Training may take a while, because we're only using a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX51TXTfe_jr"
      },
      "outputs": [],
      "source": [
        "model = NNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqmShQyVGkVf"
      },
      "outputs": [],
      "source": [
        "model.train(num_epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVfNWMiYQBTh"
      },
      "source": [
        "You should see about 30% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DheheXh8ouaW"
      },
      "outputs": [],
      "source": [
        "model.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8C7X8O2Qa0l"
      },
      "outputs": [],
      "source": [
        "# this outputs images with predicted labels\n",
        "courseutils.test_CIFAR10(model, classes, dataset_params['root'], transform, num_images=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What now?\n",
        "\n",
        "30% accuracy is still better than pure random (which should give 10% for 10 classes).\n",
        "\n",
        "But this is too low to be useful. Things you can do:\n",
        "\n",
        "*   Try different architectures (add more hidden layers) and see to what extent you can improve accuracy.  \n",
        "Find a nice way to experiment with different architectures to avoid rewriting forward() and backward() each time you change something.\n",
        "*   Implement new types of layers\n",
        "\n",
        "Of course, right now you are very limited in tools (and processing power, since we're using a CPU), so don't expect much."
      ],
      "metadata": {
        "id": "7BRoV5GBUNts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn\n",
        "\n",
        "Use our previous model as an example.\n",
        "\n"
      ],
      "metadata": {
        "id": "dlhkMTdmuc_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YourNet(courseutils.BaseNet):\n",
        "    def __init__(self, num_cls: int = 10, input_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        ## Architecture\n",
        "        self.params = {}\n",
        "        std = 1e-4\n",
        "        \n",
        "        # TODO: your layers go here.\n",
        "        # It's CRUCIAL that all of your trainable parameters\n",
        "        # stay inside self.params, e.g. self.params['W1']\n",
        "        # (this is how our base class works)\n",
        "        ...\n",
        "\n",
        "        self.num_layers = 2\n",
        "        ## params\n",
        "        self.output_activation = softmax\n",
        "        self.loss = softmax_loss\n",
        "        self.optimizer = courseutils.Optimizer(\"adam\")\n",
        "        self.loss_history = []\n",
        "\n",
        "        ## datasets\n",
        "        self.labels2names = classes\n",
        "        self.trainloader = trainloader\n",
        "        self.testloader = testloader\n",
        "        self.valloader = testloader\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> Tuple[list, dict]:\n",
        "        caches = {}\n",
        "        \n",
        "        # TODO: implement forward pass.\n",
        "        # Populate 'caches' variable with whatever you need for backward pass.\n",
        "        # It is passed directly to backward() as 'caches' argument\n",
        "        output = ...\n",
        "\n",
        "        return output, caches\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, caches: dict) -> Tuple[float, dict]:\n",
        "        grads = {}\n",
        "        loss, dOut = self.loss(X, y)\n",
        "\n",
        "        # TODO: implement backward pass.\n",
        "        # NOTE: for each trainable parameter in self.params\n",
        "        # there should be a parameter in grads with the same key,\n",
        "        # e.g. grads['W1'] is used to train self.params['W1'].\n",
        "        # otherwise our optimizer will get KeyError :(\n",
        "        ...\n",
        "\n",
        "        return loss, grads"
      ],
      "metadata": {
        "id": "b0YltjI-uhng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_model = YourNet()"
      ],
      "metadata": {
        "id": "sS-JjNxhxTqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_model.train(num_epochs=3)\n",
        "your_model.test()"
      ],
      "metadata": {
        "id": "B3o31_BQxWmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "courseutils.test_CIFAR10(your_model, classes, dataset_params['root'], transform, num_images=10)"
      ],
      "metadata": {
        "id": "iNpXvVyxxb16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}