{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2LZ8hkL-dV_x"},"outputs":[],"source":["import numpy as np\n","from typing import Tuple\n","\n","import torch\n","from tqdm import tqdm"]},{"cell_type":"markdown","source":["To begin with, we will create an architecture for reading images. Since we will implement a network with a fairly simple architecture, then the data will be greatly simplified. Namely, all images will be converted to grayscale\n","\n","In the folder with the dataset, subfolders with pictures of each class should be allocated\n","\n","CIFAR10 will be used as a training dataset. You can download it [here](http://www.cs.toronto.edu/~kriz/cifar.html) "],"metadata":{"id":"8VIKK8xJNQtf"}},{"cell_type":"code","source":["PATH_TO_DATASET = # The path to the unpacked dataset is here"],"metadata":{"id":"K1nawTGSOIci"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQ0PDWYSoUzV"},"outputs":[],"source":["from typing import Tuple, List, Union\n","\n","import albumentations as A\n","import numpy as np\n","import torch\n","import os\n","import cv2\n","import numba\n","\n","\n","def read_images(image_dir_path: str, labels2names: dict = None) -> Tuple[List[np.ndarray], List[int], dict]:\n","    names2labels = None\n","    if labels2names is None:\n","        labels2names = {}\n","    else:\n","        names2labels = {v: k for k, v in labels2names.items()}\n","    labels = []\n","    images = []\n","\n","    for i, img_dir in enumerate(os.listdir(image_dir_path)):\n","        print(img_dir)\n","        if names2labels is None:\n","            labels2names[i] = img_dir\n","        for img in os.listdir(os.path.join(image_dir_path, img_dir)):\n","            image = cv2.cvtColor(cv2.imread(os.path.join(image_dir_path, img_dir, img)), cv2.COLOR_BGR2GRAY)\n","            image[image <= 98] = 1\n","            image[image > 98] = 0\n","            images.append(crop_borders(image))\n","            if names2labels is not None:\n","                labels.append(names2labels[img_dir])\n","            else:\n","                labels.append(i)\n","\n","    return images, labels, labels2names\n","\n","\n","def crop_borders(image: np.ndarray) -> np.ndarray:\n","    try:\n","        mask = image == 0\n","\n","        coords = np.array(np.nonzero(~mask))\n","        top_left = np.min(coords, axis=1)\n","        bottom_right = np.max(coords, axis=1)\n","\n","        out = image[top_left[0] - 5:bottom_right[0] + 5, top_left[1] - 5:bottom_right[1] + 5]\n","        if out.shape[0] == 0 or out.shape[1] == 0:\n","            center_y, center_x = image.shape[0] // 2, image.shape[1] // 2\n","            return image[center_y - center_y // 2:center_y + center_y // 2,\n","                   center_x - center_x // 2: center_x + center_x // 2]\n","\n","        return out\n","    except Exception:\n","        return image\n","\n","\n","@numba.jit(nopython=True)\n","def compute_new_sample(image:np.ndarray) -> List[Union[int, np.ndarray]]:\n","    cell_width, cell_height = 2, 2\n","    result = [np.sum(image[i*cell_height, :]) for i in range(image.shape[0] // cell_height )]\n","    result.extend([np.sum(image[:, j * cell_width]) for j in range(image.shape[0] // cell_width )])\n","    return result\n","\n","\n","\n","class ImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_dir_path: str = PATH_TO_DATASET, dataset_len: int = 60000, labels2names: dict = None):\n","        images, labels, labels2names = read_images(image_dir_path, labels2names)\n","        self.images = images\n","        self.labels = labels\n","        self.labels2names = labels2names\n","        self.transform = A.Compose([\n","            #A.Downscale(p=0.3),\n","            #A.GlassBlur(p=0.3),\n","            #A.GaussianBlur(p=0.3),\n","            A.Resize(32, 32)\n","        ])\n","        self.dataset_len = dataset_len\n","\n","    def __len__(self):\n","        return self.dataset_len\n","\n","    def __getitem__(self, idx: int):\n","        idx = idx % len(self.labels)\n","        image = self.transform(image=self.images[idx])[\"image\"]\n","        # cv2.imshow(\"\", (image * 255))\n","        # cv2.waitKey()\n","        return compute_new_sample(image), self.labels[idx]\n","\n","    @property\n","    def labels2names_(self):\n","        return self.labels2names\n"]},{"cell_type":"markdown","metadata":{"id":"MQzzWtdsdKlw"},"source":["# Linear layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cejjxi2AbxGh"},"outputs":[],"source":["def linear(X: np.ndarray, W: np.ndarray, b: np.ndarray):\n","    return np.dot(X, W) + b, (X, W, b)\n","\n","def linear_backward(d: np.ndarray, prev: tuple):\n","    X, W, b = prev\n","    \"write a backwardation here. don't forget about dimensions\"\n","    dX = ...\n","    dW = ...\n","    db = ...\n","    return dX, dW, db\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E32jjcKGdvGI"},"source":["# Activation funcs"]},{"cell_type":"markdown","source":["## ReLU"],"metadata":{"id":"72ia3aU2XvTk"}},{"cell_type":"code","source":["def relu(X: np.ndarray):\n","    return np.maximum(0, X), X\n","\n","def relu_backward(d: np.ndarray, prev: np.ndarray):\n","    X = prev.copy()\n","    dx = d.copy()\n","    \"write a backwardation here\"\n","    return ..."],"metadata":{"id":"vdqv1l9BX2Cu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FV2xPEuHdiYq"},"outputs":[],"source":["def sigmoid(X: np.ndarray):\n","    return 1 / (1 + np.exp(-X))\n","\n","\n","def sigmoid_backward(d: np.ndarray, prev: np.ndarray):\n","    X = prev.copy()\n","    \"write a backwardation here\"\n","    return ..."]},{"cell_type":"markdown","metadata":{"id":"jwx0yLAid3RZ"},"source":["# Loss funcs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBOt-NuIdyXn"},"outputs":[],"source":["def mse_loss(X: np.ndarray, y: np.ndarray):\n","    preds = sigmoid(X)\n","    y_encoded = np.zeros((y.shape[0], y.max(initial=0) + 1))\n","    y_encoded[np.arange(y.shape[0]), y] = 1\n","    loss = np.mean((y_encoded - preds) ** 2)\n","    N = X.shape[0]\n","    dX = sigmoid_backward(2 * (preds - y_encoded), X)\n","    return loss, dX\n","\n","\n","def softmax(X: np.ndarray):\n","    e_x = np.exp(X - np.max(X))\n","    if len(e_x.shape) > 1:\n","        return e_x / np.sum(e_x, axis=1, keepdims=True)\n","    return e_x / np.sum(e_x)\n","\n","\n","def softmax_loss(X: np.ndarray, y: np.ndarray):\n","    logits = X - np.max(X, axis=1, keepdims=True)\n","    Z = np.sum(np.exp(logits), axis=1, keepdims=True)\n","    log_probs = logits - np.log(Z)\n","    probs = np.exp(log_probs)\n","    N = X.shape[0]\n","    loss = -np.sum(log_probs[np.arange(N), y]) / N\n","    dX = probs.copy()\n","    dX[np.arange(N), y] -= 1\n","    dX /= N\n","    return loss, dX"]},{"cell_type":"markdown","metadata":{"id":"vXKAKqnleupD"},"source":["# Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atlJqWNId6Io"},"outputs":[],"source":["class Optimizer:\n","    def __init__(self, optim_type: str = 'sgd', params: dict = None):\n","        if params is None:\n","            self.params = {}\n","        else:\n","            self.params = params\n","        self.params.setdefault('lr', 1e-3)\n","        self.params.setdefault('momentum', 0.9)\n","        self.params.setdefault(\"beta1\", 0.9)\n","        self.params.setdefault(\"beta2\", 0.999)\n","\n","        if optim_type == 'sgd':\n","            self.optimize = self.sgd\n","        elif optim_type == 'momentum':\n","            self.optimize = self.momentum\n","        elif optim_type == 'adam':\n","            self.optimize = self.adam\n","\n","    def __call__(self, W: np.ndarray, dW: np.ndarray, key: str):\n","        return self.optimize(W, dW, key)\n","\n","    def sgd(self, W: np.ndarray, dW: np.ndarray, key: str) -> np.ndarray:\n","        W -= self.params['lr'] * dW\n","        return W\n","\n","    def momentum(self, W: np.ndarray, dW: np.ndarray, key: str) -> np.ndarray:\n","        v = self.params.get(\"velocity %s\" % key, np.zeros_like(W))\n","        v = self.params[\"momentum\"] * v + self.params[\"lr\"] * dW\n","        W -= v\n","        self.params[\"velocity %s\" % key] = v\n","        return W\n","\n","    def adam(self, W: np.ndarray, dW: np.ndarray, key: str) -> np.ndarray:\n","        m = self.params.get(\"m %s\" % key, np.zeros_like(W))\n","        v = self.params.get(\"v %s\" % key, np.zeros_like(W))\n","        self.params.setdefault(\"t %s\" % key, 0)\n","        self.params['t %s' % key] += 1\n","        self.params['m %s' % key] = self.params[\"beta1\"] * m + (1 - self.params[\"beta1\"]) * dW\n","        self.params['v %s' % key] = self.params[\"beta2\"] * v + (1 - self.params[\"beta2\"]) * dW ** 2\n","        mt = self.params['m %s' % key] / (1 - np.power(self.params[\"beta1\"], self.params[\"t %s\" % key]))\n","        vt = self.params[\"v %s\" % key] / (1 - np.power(self.params[\"beta2\"], self.params[\"t %s\" % key]))\n","        W -= self.params[\"lr\"] * mt / (np.sqrt(vt) + 1e-8)\n","        return W\n"]},{"cell_type":"markdown","metadata":{"id":"VImPk9Pje8BO"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZ3yI81Ue2XD"},"outputs":[],"source":["class NNet:\n","    def __init__(self, hidden_dims: list = [500], num_cls: int = 10,\n","                 input_dim: int = 32):\n","        ## architecture\n","        # We recommend using at least 300-500 layers for this task.\n","        # To declare them, just use a loop\n","        self.params = {}\n","        std = 1e-4\n","        self.params['W1'] = std * np.random.randn(input_dim, hidden_dims[0])\n","        self.params['b1'] = np.zeros(hidden_dims[0])\n","\n","        \"\"\"Your layers are here \"\"\"\n","        ...\n","        # Don't forget that the last layer should have an output dimension \n","        # equal to the number of classes. In this case - 10\n","        self.num_layers = len(hidden_dims) + 1\n","        ## params\n","        # Don't forget to declare loss functions, optimizer, etc\n","        self.output_activation = ...\n","        self.criterion = ...\n","        self.optimizer = ...\n","        self.loss_history = []\n","\n","        ## datasets\n","        # We will still use some help from pytorch. Of course,\n","        # it would be possible to divide the data into batches by hand, but now it makes no sense\n","        traindataset = ImageDataset(dataset_len=30000)\n","        self.labels2names = traindataset.labels2names_\n","        self.trainloader = torch.utils.data.DataLoader(\n","            traindataset, batch_size=50, shuffle=True, num_workers=2\n","        )\n","\n","        self.testloader = torch.utils.data.DataLoader(\n","            ImageDataset(dataset_len=30000, labels2names=self.labels2names),\n","            batch_size=50, shuffle=True, num_workers=2\n","        )\n","        self.valloader = torch.utils.data.DataLoader(\n","            ImageDataset(dataset_len=30000, labels2names=self.labels2names),\n","            batch_size=50, shuffle=True, num_workers=2\n","        )\n","\n","    \n","\n","    def forward(self, X: np.ndarray) -> Tuple[list, dict]:\n","        # Okay, now we need to make a function\n","        # that will run the data X through all those layers that we created\n","      \n","        return ... \n","\n","    def backward(self, X: np.ndarray, y: np.ndarray, cache: dict) -> Tuple[float, dict]:\n","        # Now we need to make a backwardation. \n","        # Don't forget that you have already written functions for each layer type\n","        grads = {}\n","        loss, dOut = self.criterion(X, y)\n","\n","        return loss, grads\n","\n","    def _training_step(self, X_batch: np.ndarray, y_batch: np.ndarray, optimizer: Optimizer):\n","        # Now the training step. \n","        # Everything is simple here, do forward, \n","        # then backward and finally apply the optimizer\n","        \n","    def check_accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n","        preds = self._predict(X)\n","        preds = np.argmax(preds, axis=1)\n","        return np.mean(preds == y)\n","\n","    def train(self, num_epochs: int = 10):\n","        # Okay, now it's time to make a training loop\n","        best_params = {}\n","        best_val_acc = -1\n","        for num_epoch in range(num_epochs):\n","            running_accuracy = 0\n","            i = 0 \n","            for data in tqdm(self.trainloader):\n","                # You need to go through the data for several epochs \n","                # and perform a training step for each batch\n","                # Also, do not forget to accumulate accuracy for the subsequent evaluation of the model\n","                ...\n","                i += 1\n","            training_loss = np.mean(self.loss_history)\n","            train_acc = running_accuracy / i\n","\n","            ## validation\n","            val_accuracy = 0\n","            val_i = 0\n","            for data in tqdm(self.valloader):\n","                # During validation, you only need to look at the accuracy of the model\n","                ...\n","                val_i += 1\n","            val_acc = val_accuracy / val_i\n","            self.loss_history = []\n","\n","            print(\"%d epoch:\\n training loss: %.4f\\n \" \\\n","                  \"training accuracy: %.4f\\n validation accuracy: %.4f\" % (num_epoch + 1,\n","                                                                           training_loss,train_acc, val_acc))\n","            # If the result achieved in this era is the best, it would be nice to keep the network parameters that provided it                                                              \n","            if ...:\n","                ...\n","\n","        self.params = best_params\n","    \n","\n","    def _predict(self, X: np.ndarray):\n","        Z, _ = self.forward(X)\n","        return self.output_activation(Z[-1])\n","\n","    def predict(self, X: np.ndarray) -> str:\n","        prediction = self._predict(X)\n","        label = np.argmax(prediction)\n","        predictions = []\n","        for i, p in enumerate(prediction):\n","            predictions.append(\"%s : %.5f\" % (self.labels2names[i], p))\n","\n","        return \"Prediction: %s \\nProbabilities:\\n%s\" % (self.labels2names[label],\n","                                                        \"\\n\".join(predictions))\n","\n","    def test(self):\n","        running_accuracy = 0\n","        i = 0\n","        for data in tqdm(self.testloader):\n","            x, y = data\n","            y = y.numpy()\n","            x = torch.stack(x)\n","            x = torch.t(x).numpy()\n","            i += 1\n","            running_accuracy += self.check_accuracy(x, y)\n","        print( \"Test accuracy: %.4f\" % (running_accuracy / i))\n","        return running_accuracy / i \n"]},{"cell_type":"markdown","metadata":{"id":"Pz9zfsHle-Q2"},"source":["# Test"]},{"cell_type":"markdown","source":["Finally, we create a model, train it and test it. Of course, this is a very simple model and it is unlikely that you will get an accuracy of more than 20-25%, but it is still 2 times better than tossing a coin!"],"metadata":{"id":"GkHF16GVWD1v"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SX51TXTfe_jr"},"outputs":[],"source":["model = NNet()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqmShQyVGkVf"},"outputs":[],"source":["model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DheheXh8ouaW"},"outputs":[],"source":["assert model.test() > 0.21"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"mount_file_id":"1L7L3b-kZbBZVlHzo59VUUCApXs5HD7c6","authorship_tag":"ABX9TyN0nkITzMkbwBbF93Sfm3iD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}