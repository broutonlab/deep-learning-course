{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll learn to prect type of recipe by it's instruction! \n",
    "\n",
    "This approach can be used for automatical recipies tagging by categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.restoconnection.com/wp-content/uploads/2017/05/instagram-food.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports for all cases of life:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json, codecs, csv\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "# visualization\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk imports for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, GRU, Dropout, BatchNormalization\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read database with recipies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('./edimdoma_dataset.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "\n",
    "    names = []\n",
    "    descriptions = []\n",
    "    categories = []\n",
    "    instructions = []\n",
    "    cookTimes = []\n",
    "    for line_id, line in enumerate(reader):\n",
    "        if line_id == 0:\n",
    "            continue\n",
    "        doc = line[2]\n",
    "        recipe_info = json.loads(doc)\n",
    "\n",
    "        cookTime = -1\n",
    "        if 'cookTime' in recipe_info:\n",
    "            cookTime = recipe_info['cookTime']\n",
    "            if cookTime.startswith('PT'):\n",
    "                cookTime = cookTime[2:]\n",
    "            if cookTime.endswith('H'):\n",
    "                cookTime = cookTime[:-1]\n",
    "            if cookTime.isdigit():\n",
    "                cookTime = int(cookTime)\n",
    "                \n",
    "        if not 'recipeCategory' in recipe_info:\n",
    "            continue\n",
    "        category = recipe_info['recipeCategory'].strip() if 'recipeCategory' in recipe_info else 'NONE'\n",
    "        cuisine = recipe_info['recipeCuisine'].strip() if 'recipeCuisine' in recipe_info else 'NONE'\n",
    "        name = recipe_info['name'].strip()\n",
    "        description = recipe_info['description'].strip()\n",
    "        recipeIngredient = recipe_info['recipeIngredient']\n",
    "        recipeInstructions = [x.strip() for x in recipe_info['recipeInstructions'] if not x == None]\n",
    "\n",
    "        names.append(name)\n",
    "        descriptions.append(description)\n",
    "        categories.append(category)\n",
    "        instructions.append(' '.join(recipeInstructions))\n",
    "        cookTimes.append(cookTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print categories values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(categories)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare map category => id for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = sorted(set(categories))\n",
    "cat2id = {cat:index for index, cat in enumerate(cats)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "stop_words.extend(['.', ';', '!' ])\n",
    "print(u\" \".join(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare texts for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = <prepare sentences for word2vec training. You should: \n",
    "    1. concatenate some instructions in one big string;\n",
    "    2. Tokenize text by sentences;\n",
    "    3. Tokenize each sentence by words;\n",
    "    4. remove stop words and perform lovercasing>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, word2vec will be needed for us to train a deep model. Let's prepare it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_word2vec = <train gensim word2vec with your sentences> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review our embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trained_word2vec.similar_by_word(u'мясо'):\n",
    "    print(u\"{}: {}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trained_word2vec.similar_by_word(u'порубить'):\n",
    "    print(u\"{}: {}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to do that stuff manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "<write your own similar_by_word function>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the category classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try to prepare a model which will classify our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "input = Input(shape=(None, 300))\n",
    "out = GRU(512, return_sequences=True)(input)\n",
    "out = <Your code here>verything is great and working with Victor is a pleasure.\n",
    "out = LSTM(128)(out)\n",
    "out = Dense(len(cat2id), activation='softmax')(out)\n",
    "out = <Your code here>\n",
    "model = Model(input, out)\n",
    "model.compile(\"adam\", loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to split your dataset on train/test if you don't want to overfit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(zip(*[instructions, categories]))\n",
    "random.shuffle(dataset)\n",
    "train_dataset = dataset[:int(len(dataset)*.7)]\n",
    "test_dataset = dataset[int(len(dataset)*.7):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batches generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_count=64, batch_size=64, seq_size=20):\n",
    "    batch_x = np.zeros((batch_size, seq_size, 300))\n",
    "    batch_y = np.zeros((batch_size, len(cat2id)))\n",
    "    for bi in range(batch_count):\n",
    "        for seq_index in range(batch_size):\n",
    "            text, target = random.choice(dataset)\n",
    "            for i in range(seq_size):\n",
    "                batch_x[seq_index,i] = <initialize batch_x with prepared word2vec values>\n",
    "            batch_y[seq_index, :] = <initialize label>\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's star iterative training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    losses = []\n",
    "    for batch_x, batch_y in get_batches(train_dataset):\n",
    "        loss = model.train_on_batch(batch_x, batch_y)\n",
    "        losses.append(loss)\n",
    "    print(\"train_loss: {}\".format(np.mean(losses)))\n",
    "    \n",
    "    losses = []\n",
    "    for batch_x, batch_y in get_batches(test_dataset):\n",
    "        loss = model.test_on_batch(batch_x, batch_y)\n",
    "        losses.append(loss)\n",
    "    print(\"test_loss: {}\".format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained the our model, but we don't know how it works in production.\n",
    "Let's check it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query):\n",
    "    seq_size = 40\n",
    "    batch = np.zeros((seq_size, 300))\n",
    "    words = [x for x in word_tokenize(query) if not x in stop_words and x.isalnum()]\n",
    "    for i in range(seq_size):\n",
    "        batch[i,:] = 0\n",
    "        if i < len(words) and words[i] in trained_word2vec:\n",
    "            batch[i,:] = trained_word2vec[words[i]]\n",
    "    preds = model.predict(np.expand_dims(batch, 0))[0]\n",
    "    for x in sorted(enumerate(preds), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(u\"{}: {}\".format(cats[x[0]], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, category = test_dataset[-12]\n",
    "print(\"----------------------------\")\n",
    "print(query)\n",
    "print(\"----------------------------\")\n",
    "print(category)\n",
    "print(\"----------------------------\")\n",
    "print(\"prediction:\")\n",
    "predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, category = test_dataset[-10]\n",
    "print(\"----------------------------\")\n",
    "print(query)\n",
    "print(\"----------------------------\")\n",
    "print(category)\n",
    "print(\"----------------------------\")\n",
    "print(\"prediction:\")\n",
    "predict(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **5 points**: try to make the model significantly better. You can add any crazy stuff and tune any hyperparameters which come to mind (e.g sequence size, change number of gru/lstm layers, word2vec iterations etc etc). There is a huge number of parameters!\n",
    "2. **2 points**: learn more about batchnorm (https://arxiv.org/abs/1502.03167) and dropout (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) layers. Try to apply them here. What is better?\n",
    "3. **3 points**: solve another problem - prediction of cooking time ('cookTime' field). It's regression problem. You should to think about activation function of last layer and loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
