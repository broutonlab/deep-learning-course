{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QG7Wzod88j5"
      },
      "outputs": [],
      "source": [
        "#!pip install \"git+https://github.com/broutonlab/deep-learning-course.git\"\n",
        "\n",
        "import dl_course"
      ],
      "id": "7QG7Wzod88j5"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configure notebook (if this cell fails, just launch it again)\n",
        "\n",
        "from dl_course.optimizers.utils import *\n",
        "from dl_course.optimizers.visual import *\n",
        "optim_install_dependencies()\n",
        "optim_configure_notebooks()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZVSRWtt1Utza"
      },
      "id": "ZVSRWtt1Utza",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# remove this if you are not using dark theme\n",
        "plt.style.use('dark_background')"
      ],
      "metadata": {
        "id": "FNd2LkTdoXoi"
      },
      "id": "FNd2LkTdoXoi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa78d6fd"
      },
      "source": [
        "## Loss function visualization\n",
        "\n",
        "As discussed previously, **loss function** is a way to evaluate how well a model (defined by **a set of parameters**) performs based on ground truth values provided by training dataset.\n",
        "\n",
        "Imagine our model has two parameters. This allows us to plot a 3D surface (or a heatmap), where XY plane is the space of all possible combinations of parameters, and Z axis shows loss function values for each of the points in the parameter space.\n",
        "Of course, actual ML tasks have thousands of parameter dimensions, and not just two, but there's no way we could visualize that. Two-parameter case is used to help to develop intuition about different optimization algorithms.\n",
        "\n",
        "Remember that our task is to minimize the loss function, i.e. in this case, find (x, y) coordinates of global minimum."
      ],
      "id": "aa78d6fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88242b0f"
      },
      "source": [
        "## Consider this loss function landscape:"
      ],
      "id": "88242b0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "321e3201",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# benchmark function for testing optimization algorithms\n",
        "# https://en.wikipedia.org/wiki/Rosenbrock_function\n",
        "def rosenbrock(x1x2, a=1, b=100):\n",
        "    x1, x2 = x1x2\n",
        "    return b*(x2-x1**2)**2+(a-x1)**2\n",
        "    \n",
        "visualize_3d(rosenbrock, xlim=(-7, 10), elev=45, azim=-120)"
      ],
      "id": "321e3201"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20a4fd85"
      },
      "source": [
        "### All optimizer functions in this notebook have following API:\n",
        "\n",
        "Input:\n",
        "- w: numpy array of current weights (of size 2 in our case)\n",
        "- dw: numpy array of gradients with respect to w\n",
        "- params: parameters such as learning rate, momentum, etc.\n",
        "\n",
        "Returns:\n",
        "- next_w: next predicted position which should optimize the loss\n",
        "- params: dictionary of parameters for the next update (some optimizers have a state which updates with each iteration)"
      ],
      "id": "20a4fd85"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d474b2f"
      },
      "source": [
        "## Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Implement SGD in the cell below."
      ],
      "id": "9d474b2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34732e80"
      },
      "outputs": [],
      "source": [
        "def SGD(w, dw, **params):\n",
        "    lr = params['learning_rate']\n",
        "    \n",
        "    # ========================\n",
        "    # [!] TODO: implement SGD\n",
        "\n",
        "    # ========================\n",
        "    \n",
        "    return next_w, params"
      ],
      "id": "34732e80"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf75736"
      },
      "source": [
        "## Test your SGD implementation\n",
        "\n",
        "Global minimum of our Rosenbrock function should be around **(1, 1)**.\n",
        "Final loss should be **0**."
      ],
      "id": "ecf75736"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7983c5cb"
      },
      "outputs": [],
      "source": [
        "rosenbrock_start = [8., 0.]\n",
        "num_iter = 1000\n",
        "\n",
        "# test your SGD\n",
        "path_SGD = run_optim(w=rosenbrock_start, \n",
        "          f=rosenbrock, \n",
        "          optimizer=SGD, \n",
        "          n_iter=num_iter, \n",
        "          learning_rate=1e-4)\n",
        "\n",
        "print_convergence(path_SGD)"
      ],
      "id": "7983c5cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d1d4e95"
      },
      "outputs": [],
      "source": [
        "anim = visualize_3d(rosenbrock, \n",
        "                    paths=[path_SGD], \n",
        "                    colors=[\"green\"], \n",
        "                    xlim=(-7, 10), elev=45, azim=-120)\n",
        "anim"
      ],
      "id": "6d1d4e95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11251a6"
      },
      "source": [
        "## SGD+Momentum\n",
        "\n",
        "Implement SGD+momentum in the cell below."
      ],
      "id": "f11251a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04251728"
      },
      "outputs": [],
      "source": [
        "def SGD_momentum(w, dw, **params):\n",
        "    lr = params['learning_rate']\n",
        "    momentum = params['momentum']\n",
        "    velocity = params.get(\"velocity\", np.zeros_like(w))\n",
        "    \n",
        "    # ========================\n",
        "    # [!] TODO: implement SGD+momentum\n",
        "    \n",
        "    \n",
        "    # ========================\n",
        "    \n",
        "    params[\"velocity\"] = velocity\n",
        "    \n",
        "    return next_w, params"
      ],
      "id": "04251728"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "688813f1"
      },
      "source": [
        "## Test your SGD+Momentum implementation\n",
        "\n",
        "It is recommended that you try different learning rate and compare the results."
      ],
      "id": "688813f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22a91449"
      },
      "outputs": [],
      "source": [
        "path_SGD_momentum = run_optim(w=rosenbrock_start, \n",
        "          f=rosenbrock, \n",
        "          optimizer=SGD_momentum, \n",
        "          n_iter=num_iter, \n",
        "          learning_rate=1e-4,\n",
        "          momentum=0.7)\n",
        "\n",
        "print_convergence(path_SGD_momentum)"
      ],
      "id": "22a91449"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02c4d7dd"
      },
      "outputs": [],
      "source": [
        "anim = visualize_3d(rosenbrock, \n",
        "                    paths=[path_SGD_momentum, path_SGD], \n",
        "                    colors=[\"red\", \"green\"], \n",
        "                    xlim=(-7, 10), elev=45, azim=-120)\n",
        "anim"
      ],
      "id": "02c4d7dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37213a81"
      },
      "outputs": [],
      "source": [
        "plot_losses([path_SGD_momentum, path_SGD], colors=[\"red\", \"green\"])"
      ],
      "id": "37213a81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd036442"
      },
      "source": [
        "## Implement RMSProp\n",
        "\n",
        "Implement RMSProp in the cell below."
      ],
      "id": "fd036442"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a8453d9"
      },
      "outputs": [],
      "source": [
        "def RMSProp(w, dw, **params):\n",
        "    lr = params['learning_rate']\n",
        "    decay_rate = params['decay_rate']\n",
        "    epsilon = params['epsilon']\n",
        "    cache = params.get(\"cache\", np.zeros_like(w))\n",
        "    \n",
        "    # ========================\n",
        "    # [!] TODO: implement RMSProp\n",
        "    \n",
        "\n",
        "    # ========================\n",
        "    \n",
        "    params[\"cache\"] = cache\n",
        "    \n",
        "    return next_w, params"
      ],
      "id": "8a8453d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ee37c3"
      },
      "source": [
        "## Test your RMSProp implementation\n",
        "\n",
        "We'll be using a different function, which has not one, but 4 global minima.\n",
        "Values at these minima are still **0**."
      ],
      "id": "f4ee37c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c90eb29d"
      },
      "outputs": [],
      "source": [
        "# https://en.wikipedia.org/wiki/Himmelblau%27s_function\n",
        "def himmelblau(x1x2):\n",
        "    x1, x2 = x1x2\n",
        "    return (x1**2+x2-11)**2 + (x1+x2**2-7)**2\n",
        "\n",
        "himmelblau_start = [-0.27, -0.923]"
      ],
      "id": "c90eb29d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8f0390e"
      },
      "outputs": [],
      "source": [
        "path_RMSProp = run_optim(w=himmelblau_start, \n",
        "          f=himmelblau, \n",
        "          optimizer=RMSProp, \n",
        "          n_iter=num_iter, \n",
        "          learning_rate=1e-1,\n",
        "          decay_rate=0.9,\n",
        "          epsilon=1e2)\n",
        "\n",
        "print_convergence(path_RMSProp)"
      ],
      "id": "e8f0390e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c6b2b6b"
      },
      "outputs": [],
      "source": [
        "anim = visualize_3d(himmelblau, \n",
        "                    paths=[path_RMSProp], \n",
        "                    colors=[\"yellow\"], \n",
        "                    xlim=(-4.5, 4.5), elev=50, azim=10)\n",
        "anim"
      ],
      "id": "4c6b2b6b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement AdaGrad\n",
        "\n",
        "Implement AdaGrad in the cell below."
      ],
      "metadata": {
        "id": "DZRMtkwf58tD"
      },
      "id": "DZRMtkwf58tD"
    },
    {
      "cell_type": "code",
      "source": [
        "def AdaGrad(w, dw, **params):\n",
        "    lr = params['learning_rate']\n",
        "    decay_rate = params['decay_rate']\n",
        "    epsilon = params['epsilon']\n",
        "    cache = params.get(\"cache\", np.zeros_like(w))\n",
        "    \n",
        "    # ========================\n",
        "    # [!] TODO: implement AdaGrad\n",
        "\n",
        "\n",
        "    # ========================\n",
        "    \n",
        "    params[\"cache\"] = cache\n",
        "    \n",
        "    return next_w, params"
      ],
      "metadata": {
        "id": "TptPDfBd6Du9"
      },
      "id": "TptPDfBd6Du9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_AdaGrad = run_optim(w=himmelblau_start, \n",
        "          f=himmelblau, \n",
        "          optimizer=AdaGrad, \n",
        "          n_iter=num_iter, \n",
        "          learning_rate=1e-1,\n",
        "          decay_rate=0.9,\n",
        "          epsilon=1e1)\n",
        "\n",
        "print_convergence(path_AdaGrad)"
      ],
      "metadata": {
        "id": "Ld1sEVPY7w1Z"
      },
      "id": "Ld1sEVPY7w1Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Adam\n",
        "\n",
        "Implement Adam in the cell below."
      ],
      "metadata": {
        "id": "flcxdvvh_aGf"
      },
      "id": "flcxdvvh_aGf"
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam(w, dw, **params):\n",
        "    lr = params['learning_rate']\n",
        "    beta1 = params['beta1']\n",
        "    beta2 = params['beta2']\n",
        "    epsilon = params['epsilon']\n",
        "    m = params.get(\"m\", np.zeros_like(w))\n",
        "    v = params.get(\"v\", np.zeros_like(w))\n",
        "    t = params.get(\"t\", 0)\n",
        "    \n",
        "    # ========================\n",
        "    # [!] TODO: implement Adam\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ========================\n",
        "    \n",
        "    # ========================\n",
        "    # [!] TODO: update parameters for the next iteration\n",
        "\n",
        "\n",
        "\n",
        "    # ========================\n",
        "    \n",
        "    return next_w, params"
      ],
      "metadata": {
        "id": "gzUkLB8o_kid"
      },
      "id": "gzUkLB8o_kid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_Adam = run_optim(w=himmelblau_start, \n",
        "          f=himmelblau, \n",
        "          optimizer=Adam, \n",
        "          n_iter=num_iter, \n",
        "          learning_rate=1e-2,\n",
        "          beta1=0.9,\n",
        "          beta2=0.999, \n",
        "          epsilon=1e-8)\n",
        "\n",
        "print_convergence(path_Adam)"
      ],
      "metadata": {
        "id": "DSbukxvSATvY"
      },
      "id": "DSbukxvSATvY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try some state of the art optimizers\n",
        "\n",
        "We'll be using RangerQH optimizer for this one."
      ],
      "metadata": {
        "id": "geFqGPOrGFDt"
      },
      "id": "geFqGPOrGFDt"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_optimizer as optim\n",
        "\n",
        "path_Ranger = run_torch_optim(w=himmelblau_start, \n",
        "                              f=himmelblau,\n",
        "                              optimizer_cls=optim.RangerQH,\n",
        "                              lr=1e-1,\n",
        "                              n_iter=num_iter)\n",
        "\n",
        "print_convergence(path_Ranger)"
      ],
      "metadata": {
        "id": "kVkVo80UGnG8"
      },
      "id": "kVkVo80UGnG8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's plot all of our results!"
      ],
      "metadata": {
        "id": "UnrSbpKnCL7D"
      },
      "id": "UnrSbpKnCL7D"
    },
    {
      "cell_type": "code",
      "source": [
        "# running your functions on the new landscape\n",
        "path_SGD_himmelblau = run_optim(w=himmelblau_start, \n",
        "          f=himmelblau, \n",
        "          optimizer=SGD, \n",
        "          n_iter=num_iter, \n",
        "          learning_rate=1e-3)\n",
        "\n",
        "path_SGD_momentum_himmelblau = run_optim(w=himmelblau_start, \n",
        "          f=himmelblau, \n",
        "          optimizer=SGD_momentum, \n",
        "          n_iter=num_iter, \n",
        "          learning_rate=1e-3,\n",
        "          momentum=0.9\n",
        "          )"
      ],
      "metadata": {
        "id": "mF_vAlSLD0HO"
      },
      "id": "mF_vAlSLD0HO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = [path_Adam, \n",
        "         path_SGD_himmelblau, \n",
        "         path_SGD_momentum_himmelblau, \n",
        "         path_RMSProp,\n",
        "         path_AdaGrad,\n",
        "         path_Ranger]\n",
        "\n",
        "colors = [\"cyan\",\n",
        "          \"green\",\n",
        "          \"red\",\n",
        "          \"yellow\",\n",
        "          \"purple\",\n",
        "          \"orange\"]\n",
        "\n",
        "anim = visualize_3d(himmelblau, \n",
        "                    paths=paths, \n",
        "                    colors=colors, \n",
        "                    xlim=(-4.5, 4.5), elev=80, azim=10)\n",
        "anim"
      ],
      "metadata": {
        "id": "YxNaeqlsCO7M"
      },
      "id": "YxNaeqlsCO7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(paths, colors=colors)"
      ],
      "metadata": {
        "id": "BAJ4P756D_G-"
      },
      "id": "BAJ4P756D_G-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home task\n",
        "\n",
        "- try implementing one of the state-of-the-art optimizers \n",
        "(you may find this useful: https://danielhanchen.github.io/optimizers/sota%20optimizers.htm)\n",
        "- try benchmarking your optimizers using the function below. Its global minimum is **0** at **(0, 0)**\n",
        "- try programmatically tuning parameters such as learning rate (use path['best_loss'])"
      ],
      "metadata": {
        "id": "nqUvJYEeMYq6"
      },
      "id": "nqUvJYEeMYq6"
    },
    {
      "cell_type": "code",
      "source": [
        "def ackley(x1x2):\n",
        "  x1, x2 = x1x2\n",
        "  exp, sqrt, cosine = np.exp, np.sqrt, np.cos\n",
        "  if torch.is_tensor(x1x2):\n",
        "    exp, sqrt, cosine = torch.exp, torch.sqrt, torch.cos\n",
        "    \n",
        "  return -20.0 * exp(-0.2 * sqrt(0.5 * (x1**2 + x2**2))) - \\\n",
        "         exp(0.5 * (cosine(2 * np.pi * x1) + cosine(2 * np.pi * x2))) + np.e + 20\n",
        "\n",
        "ackley_start = [-10., -10.]\n",
        "\n",
        "visualize_3d(ackley, xlim=(-20, 20), elev=10, azim=-120)"
      ],
      "metadata": {
        "id": "RhglqB10NyN3"
      },
      "id": "RhglqB10NyN3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}