{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "lab_ner.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sX853Ypqp7r"
      },
      "source": [
        "# ![img](https://broutonlab.com/static/img/banners/data-extraction-and-document-parsing-software.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HzH3xIWAGhv"
      },
      "source": [
        "In this notebook we will work with [MIT Movie Corpus](https://groups.csail.mit.edu/sls/downloads/movie/).\n",
        "\n",
        "```\n",
        "The MIT Movie Corpus is a semantically tagged training and test corpus in BIO format. \n",
        "The eng corpus are simple queries, and the trivia10k13 corpus are more complex queries.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liLUmXRTCu9W"
      },
      "source": [
        "# **Load dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Yn4lhzyYOa"
      },
      "source": [
        "Let's load train/test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t50oOSEYC1a9"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "def load_and_process_data(data_url, dest_file_path):\n",
        "  urllib.request.urlretrieve(data_url, dest_file_path)\n",
        "  with open(dest_file_path) as f:\n",
        "      text = f.read()\n",
        "  \n",
        "  dataset = []\n",
        "  for item in text.split('\\n'):\n",
        "      item = item.strip()\n",
        "      if len(item) == 0:\n",
        "          continue\n",
        "      [t, w] = item.split('\\t')\n",
        "      dataset.append((w, t))\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4SUgEu5DK4f"
      },
      "source": [
        "train_dataset = load_and_process_data(\"https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio\", \"./engtrain.bio\")\n",
        "test_dataset = load_and_process_data(\"https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio\", \"./engtest.bio\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHNwDH3tydaK"
      },
      "source": [
        "... and prepare list of all tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrvcK7k4yc4U"
      },
      "source": [
        "types = list(set(map(lambda x: x[1], train_dataset + test_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDrzdfPoCWsl"
      },
      "source": [
        "# **Let's try pretrained Spacy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGZ_AOY8yp_n"
      },
      "source": [
        "There are a number of NER frameworks. Let's try to use Spacy to solve this problem!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3Vhg76FB0XR"
      },
      "source": [
        "import spacy\n",
        "spacy_pretrained_model = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def debug_spacy(spacy_model, snt):\n",
        "  doc = spacy_model(snt)\n",
        "  for ent in doc.ents:\n",
        "      print(\"{} [{}-{}]: {}\".format(ent.text, ent.start_char, ent.end_char, ent.label_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URHNf67ZB2yP"
      },
      "source": [
        "debug_spacy(spacy_pretrained_model, \"I live in Russia. I work in ABC LLC.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPtryGYsFLn9"
      },
      "source": [
        "debug_spacy(spacy_pretrained_model, \" \".join(map(lambda x: x[0], train_dataset[:150])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jguNeb2qHxk4"
      },
      "source": [
        "Not bad, but seems to be it extracts standard entities like persons, locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtHbm_XNy3LG"
      },
      "source": [
        "We have checked that Spacy does not solve the problem and magic has not happen. Let's train our NER system in plain Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRgxQfa6ZWfQ"
      },
      "source": [
        "# **NER in Keras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEO3jQBkzYj5"
      },
      "source": [
        "In this tutorial we will have have a deal with word-based approach and we will see how it works.\n",
        "First of all we have to prepare dictionary of input words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeBbNTI6qp77"
      },
      "source": [
        "from collections import Counter\n",
        "word2count = Counter(map(lambda x: x[0], train_dataset))\n",
        "MAX_WORD_COUNT = 50000\n",
        "top_words = [x[0] for x in sorted(word2count.items(), key=lambda x: x[1], reverse=True)][:MAX_WORD_COUNT]\n",
        "word2id = {x:index+1 for index, x in enumerate(top_words)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmV_XnhrzpPp"
      },
      "source": [
        "Let's implement NER model. Don't hesitate to modify it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2TR14n5qp8A"
      },
      "source": [
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.models import Model\n",
        "\n",
        "input = Input(shape=(None,))\n",
        "out = Embedding(input_dim=len(word2id)+1, output_dim=200)(input)\n",
        "# your code\n",
        "out = Bidirectional(LSTM(200, activation='relu', return_sequences=True))(out)\n",
        "out = Dense(len(types), activation='softmax')(out)\n",
        "model = Model(input, out)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrn0dyDRqp8C"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "type2id = {x:index for index, x in enumerate(types)}\n",
        "print(type2id)\n",
        "\n",
        "def getWordId(w):\n",
        "    return 0 if not w in word2id else word2id[w]\n",
        "\n",
        "def gen_batches(dataset, batch_size=64, seq_size=32, batch_count=100):\n",
        "    random.shuffle(dataset)\n",
        "    \n",
        "    features = np.zeros((batch_size, seq_size))\n",
        "    labels = np.zeros((batch_size, seq_size, len(type2id)))\n",
        "    for _ in range(batch_count):\n",
        "        for seq_index in range(batch_size):\n",
        "            left = random.randint(0, len(dataset) - seq_size)\n",
        "            features[seq_index,:] = [getWordId(x[0]) for x in dataset[left:left+seq_size]]\n",
        "            labels[seq_index,:] = 0\n",
        "            for i,(_,t) in enumerate(dataset[left:left+seq_size]):\n",
        "                labels[seq_index,i] = 0\n",
        "                labels[seq_index,i,type2id[t]] = 1\n",
        "        yield features, labels\n",
        "\n",
        "def gen_data(dataset, seq_size=32, item_count=100):\n",
        "  random.shuffle(dataset)\n",
        "    \n",
        "  labels = np.zeros((seq_size, len(type2id)))\n",
        "  for _ in range(item_count):\n",
        "    left = random.randint(0, len(dataset) - seq_size)\n",
        "    features = np.array([getWordId(x[0]) for x in dataset[left:left+seq_size]])\n",
        "    labels[:] = 0\n",
        "    for i,(_,t) in enumerate(dataset[left:left+seq_size]):\n",
        "      labels[i] = 0\n",
        "      labels[i,type2id[t]] = 1\n",
        "    yield features, labels\n",
        "        \n",
        "def encode_text(sentence):\n",
        "    words = sentence.split()\n",
        "    result = np.zeros((len(words),))\n",
        "    for i,w in enumerate(words):\n",
        "        result[i] = getWordId(w)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtnfOkTQlCsV"
      },
      "source": [
        "x_val, y_val = zip(*list(gen_batches(test_dataset, batch_count=150)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5tnbRXxho1Q"
      },
      "source": [
        "from datetime import datetime\n",
        "import keras.callbacks\n",
        "\n",
        "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "mcp_save = keras.callbacks.ModelCheckpoint('trained_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "training_history = model.fit(\n",
        "    gen_batches(train_dataset, batch_count=1000),\n",
        "    validation_data=(x_val, y_val),\n",
        "    verbose=1, steps_per_epoch=10, epochs=32,\n",
        "    callbacks=[tensorboard_callback, mcp_save])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9n4-L3KvE-r"
      },
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir=logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsf_EeDpqp8K"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELJYNCEnqp8K"
      },
      "source": [
        "Let's review how model works in production!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0BQATklx8zW"
      },
      "source": [
        "from tensorflow import keras\n",
        "best_model = keras.models.load_model('trained_model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE_WWqKEqp8L"
      },
      "source": [
        "query = test_dataset[200:300]\n",
        "query_words = [x[0] for x in query]\n",
        "query_types = [x[1] for x in query]\n",
        "result = best_model.predict_on_batch(encode_text(\" \".join(query_words)).reshape((1, -1)))[0]\n",
        "predictions = []\n",
        "import pandas as pd\n",
        "\n",
        "for index in range(result.shape[0]):\n",
        "    w = query_words[index]\n",
        "    true_type = query_types[index]\n",
        "    pred_type = types[np.argmax(result[index,:])] \n",
        "    predictions.append(pred_type)\n",
        "\n",
        "result_dataframe = pd.DataFrame.from_dict({\"words\":query_words, \"types\":query_types, \"preds\": predictions})\n",
        "result_dataframe.head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdTdaEWIxTJN"
      },
      "source": [
        "# Char based approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cRzyLmsqp8N"
      },
      "source": [
        "# Home task\n",
        "\n",
        "- 3 points: make the model better\n",
        "- 7 points: implement the model with CRF layer (https://github.com/Hironsan/keras-crf-layer)"
      ]
    }
  ]
}